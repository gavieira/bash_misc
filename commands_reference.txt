-- Brace expansion

Used to select multiple files in a single dir, for example

rm -rf abc.log.2012-03-{14,27,28}

cp /home/usr/dir/{file1,file2,file3,file4} /home/usr/destination/

-- linux bang commands

!!    # Repeat last command
!:0   # Returns command used
!^    # Reapeats first argument from previous command
!$    # Repeats the last argument from previous command
!:2   # Repeats second argument from previous command
!:10-12  # Repeats from 10th to 12th arguments
!n    # Return command number n from history
!pattern  # Most recent command matching pattern
!!:s/find/replace   # last command, substitute find with replace

Much more can be done with these. Look these links for more: 

https://www.redhat.com/sysadmin/bash-bang-commands#:~:text=Under%20the%20hood%2C%20bang%20(!),in%20your%20previous%20commands%20quickly.

https://stackoverflow.com/questions/3371294/how-can-i-recall-the-argument-of-the-previous-bash-command


--git

#restore all files to previous commit
git reset --hard

#restore a single file to previous commit
git checkout -- path/to/foo

-- bash globbing (filename expansion/pattern matching) tutorial:

https://linuxhint.com/bash_globbing_tutorial/

--lsof

list open files for a given path, as well as associated PIDs

lsof /home/user/

--killall

Kill all processes with a given name (e.g. rstudio)

killall rstudio

--hmmer

Getting PFAM HMM profile database:

ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz


Creating profile from alignment (hmmbuild):

Usage: hmmbuild [-options] <hmmfile output> <alignment file>

hmmbuild custom_data.hmm custom_data.aln

Obs: "Normally the ID line is optional in Stockholm format, but hmmbuild has to name your new profile(s) somehow. For a single alignment, it will use your filename, or you can use the hmmbuild -n <name> option to provide a name yourself. For an alignment database, the only way hmmbuild can get a name for each alignment is from alignment annotation. Of alignment file formats, only Stockholm format provides a way to concatenate many alignments in the same file, with a name for each alignment." (HMMER manual)


Make multiple sequence alignment using a profile (hmmalign):

Usage: hmmalign [-options] <hmm file> <sequence file> (Aligns all seqs to the profile

hmmalign globins4.hmm globins45.fa


Search profile against sequence database (hmmsearch):

Usage: hmmsearch [-options] <hmmfile> <sequence file or database>

hmmsearch globins4.hmm uniprot_sprot.fasta > globins4.out


Prepare profile database for hmmscan (hmmpress):

Usage: hmmpress [-options] <hmmfile>


Search sequence against profile database (hmmscan):

hmmscan [-options] <hmmdb> <seqfile>

Options controlling output:
  -o <f>           : direct output to file <f>, not stdout
  --tblout <f>     : save parseable table of per-sequence hits to file <f>
  --domtblout <f>  : save parseable table of per-domain hits to file <f>
  --pfamtblout <f> : save table of hits and domains to file, in Pfam format <f>
  --acc            : prefer accessions over names in output
  --noali          : don't output alignments, so output is smaller
  --notextw        : unlimit ASCII text output line width
  --textw <n>      : set max width of ASCII text output lines  [120]  (n>=120)


--rsync

Get file(s) from server (custom port):

rsync -avzh -e 'ssh -p custom_port' user@ip:/path/to/remote/files /path/to/save/in/local


--conda

# Add Bioconda to software channels

conda config --add channels defaults && conda config --add channels bioconda && conda config --add channels conda-forge


--youtube-dl

# Download all mp3 playlists of YouTube channel/user keeping each playlist in separate directory:

youtube-dl -i -x --add-metadata --embed-thumbnail --audio-format mp3 -o '%(uploader)s/%(playlist)s/%(title)s.%(ext)s' https://www.youtube.com/channel/UC-zQnWIPHqhJm4NdjQmPy5g/playlists

# Download single playlist in a new directory (ordered and with autosubs, ignoring errors):

youtube-dl -f best -o "%(playlist)s/%(autonumber)s-%(title)s.%(ext)s" -i --write-auto-sub <playlist url>

--jupyter-lab (open remotely) - https://amber-md.github.io/pytraj/latest/tutorials/remote_jupyter_notebook

In remote host, open the terminal, change directory to where you have your notebooks and type:

jupyter notebook --no-browser --port=8889
#Leave it open

In local computer, type (port forwarding):

ssh -N -f -L localhost:8888:localhost:8889 username@your_remote_host_name

Now you can access the remote host's jupyter-lab at:

localhost:8888

--jupyter notebook convert (nbconvert)

    The simplest way to use nbconvert is

    > jupyter nbconvert mynotebook.ipynb

    which will convert mynotebook.ipynb to the default format (probably HTML).

    You can specify the export format with `--to`.
    Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides']

    > jupyter nbconvert --to latex mynotebook.ipynb

    Both HTML and LaTeX support multiple output templates. LaTeX includes
    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You
    can specify the flavor of the format used.

    > jupyter nbconvert --to html --template basic mynotebook.ipynb


-- symbolic links (symlinks)

#To create, use:

ln -s {source-filename} {symbolic-filename}

-- docker

#Create container from image:

docker run --name teste conda/miniconda3 /bin/bash

#Run container in interactive shell:

docker start -i teste

#Remove containers:

docker container rm container1 container2

#Remove images:

docker image rm image1 image2

#Run container with $PWD as volume:

docker run -d -v $PWD:/mnt -w /mnt  container_name command_line

#Stop/remove all of Docker containers:

docker stop $(docker ps -a -q)
docker rm $(docker ps -a -q)

or:

docker rm -f $(docker ps -a -q) #Both stops/removes

#Useful link: https://www.digitalocean.com/community/tutorials/how-to-remove-docker-images-containers-and-volumes

##Remove all images:

docker rmi $(docker images -a -q)

##Remove dangling (not associated with container) images, volumes, etc:

docker system prune

##Remove everything:

docker system prune -a

-- mitos

runmitos.py -i <fasta_file> -c <genetic_code> -o <output_dir> -r <reference_dir>

Obs: Reference dir can be found at https://zenodo.org/record/2672835#.XhsxXJnQ_iw

--bash for loop

#When all files of interest are in the same dir, you don't need to use 'ls' or 'find' to get a list of them.

for i in /path/to/dir/*.gb; do echo $i; done    #Will return all files from a given directory with a '.gb' extension

#When files are spread throughout several folders, using the find command may be a good idea, But you will have to change the IFS (Internal Field Separator) to parse its output

IFS=$'\n'; for i in $(find . -name *.fa); do echo "This is the file" $i; done && unset IFS  #Find all files with .fa extension and can perform operations on them


#Doing the same with directories:

IFS=$'\n'; for i in $(find . -type d -name "mitos_results" -print); do echo "This is the directory" $i; done && unset IFS  #Find all "mitos_results" directories  and perform operations on them


#You can also get the absolute path to directories, go inside them, and do whatever you want there: 

for i in $PWD/*/; do echo "This is the folder" $i; done

#For instance, you can remove all files except by those with some specific extension:

for i in $PWD/*/; do cd $i && rm -rf !(*extension); done

-- find
#Obs: In bash - Tilde Expansion: Gets absolute path of the current working directory

echo ~+


#Print absolute path for all occurences of filename within current directory:

find ~+ -name filename

#Or, alternatively (bash not needed):

find "$(pwd -P)" -name "filename"

#Recursively get path to directory

find YOUR_STARTING_DIRECTORY -type d -name "*99966*" -print

-- Command Substitution: Storing the output of a command in a variable. There are two ways to do this:

#With $(comand) notation

echo $(ls)
total_seqs=$(grep -c ">" Trinity.120.fasta)

#With backticks `command`

echo `ls`
total_seqs=`grep -c ">" Trinity.120.fasta`

-- rm

#Remove everything except certain files/patterns ( more on Extended Pattern Matching Operators at https://www.linuxjournal.com/content/pattern-matching-bash and specifically about using them with rm at https://www.tecmint.com/delete-all-files-in-directory-except-one-few-file-extensions/ )

#To enable extended pattern operators, enable the extglob shell:
shopt -s extglob

#Them, remove all files, except a few, with:

rm !(file1.txt|file2.zip|file3.jpg)

#Finally, remove the extglob shell:

shopt -u extglob

#Alternatively:

shopt -s extglob && rm !(file1.txt|file2.zip|file3.jpg) && shopt -u extglob

--cut

Mandatory arguments to long options are mandatory for short options too.
  -b, --bytes=LIST        select only these bytes
  -c, --characters=LIST   select only these characters
  -d, --delimiter=DELIM   use DELIM instead of TAB for field delimiter
  -f, --fields=LIST       select only these fields;  also print any line
                            that contains no delimiter character, unless
                            the -s option is specified
  -n                      (ignored)
      --complement        complement the set of selected bytes, characters
                            or fields
  -s, --only-delimited    do not print lines not containing delimiters
      --output-delimiter=STRING  use STRING as the output delimiter
                            the default is to use the input delimiter
  -z, --zero-terminated    line delimiter is NUL, not newline
      --help     display this help and exit
      --version  output version information and exit

#Imprimir apenas alguns campos do arquivo original (cut assume TAB como delimiter padrão):

cut -f1,2,5,6 --complement PFERALIS_ARTEMIS.gff

#Excluir campo (coluna) do arquivo original:

cut -f4 --complement PFERALIS_ARTEMIS.gff

#String slicing with cut:

echo "morango" | cut -c2 #Gets the second character ("o")

echo "morango" | cut -c2-5 #Gets from second to fifth character ("oran")

echo "morango" | cut -c2- #Gets from second to last character ("orango")

echo "morango" | cut -c-5 #Gets from second to fifth character ("moran")

#Getting ids for multiple fasta files with cut (with ">"):

for i in *fasta; do header=$(cut -d '>' -f 1,2 -s $i) && echo $header; done
-d '>' = sets '>' as separator
-f 1,2 = returns both separated fields ('>', sequence_id)
-s = prints only the lines that have the separator character

#Getting ids for multiple fasta files with cut (without ">"):

for i in *fasta; do header=$(cut -d '>' -f 2 -s $i) && echo $header; done

--awk

#Substituir texto em uma unica coluna:

awk -F"\t" '{gsub("gene","CDS",$3); print}' OFS="\t"  PPERBOSCII_ARTEMIS.gff > NEW.gff

Troca a string "gene" por "CDS" na coluna 3, usando tab ("\t") como separador.


#Imprimir linhas em que uma coluna seja >= x:
awk '$4>=2{print}' filename (Imprme linhas nas quais a coluna 4 tenha valor >= a 2)


#pegar uma sequencia de um arquivo multifasta com base no ID:

awk -vRS=">" 'BEGIN{t["ID da sequencia"]=1} {if($1 in t){printf ">%s",$0}}'  sequence.fasta

Cria objeto t, e se esse objeto está na linha, ele o imprime até o próximo separador (>).


#Get system usernames:

awk -F: '{print $1}' /etc/passwd


#Get groups and associated users:

awk -F: '{print $1, "-", $NF}' /etc/group

NF corresponds to "Number of Fields". Thus, this oneliner gets the first and last fields.

--sed

# Gerar arquivos com apenas as linhas de 1 a 125 do arquivo original:

sed -n 1,125p input.file > output.file

#Multiple line ranges:

sed -n '2,4p;6,8p' file.txt

#Substituir pattern sem abrir arquivo:

sed -i -e 's/few/asd/g' hello.txt

s/ substitui 'few' por 'asd' no arquivo hello.txt
/g stands for "global" - substitui todos os 'few' na msma linha

sed "s/'/\"/g"  Input_file -  troca single quote por double quote

--ls

#Listar apenas pastas:
ls -ld */

#listar apenas um arquivo por linha:
ls -1

#Listar continuamente (refresh 3 segundos)
watch -n3 ls -ltrh

watch - repete o comando.


# Parar processo, colocar no background e adicionar nohup:
Ctrl+Z (para o processo)
bg (coloca no background - &)
job (para ver o número do job)
disown -h %[nº_do_job - coloca o nohup
ex: disown -h %1


--tar

#Comprimir:

tar -cjvf files.tar.bz2 stuff => comprime para .tar.bz2 (+ lento, mas mais compactado)


tar -czvf files.tar.gz /usr/local/something => comprime para .tar.gz (+rápido e menos compacto)

tar -cakvf files.tar.bz2 /usr/local => Identifica o programa para compressão a partir da extensão do arquivo (no caso, bz2). Mantém o arquivo original

-c : create - cria um arquivo (não deleta o anterior_
-z: comprime com gzip
-j: comprime bzip2
-a: identifica qual compressão usar baseado na extensão do arquivo
-f: vc especifica o nome do arquivo de saida
-k: mantém arquivo original

tar -cavf files.tar.gz /home/ubuntu --exclude='/home/ubuntu/Downloads' --exclude='/home/ubuntu/.cache' => Comprime tudo exceto (aceita wildcard)


--tail

#Ver arquivos de log em tempo real (20 linhas)

tail -n 20 -f nome_do_arquivo.txt


--uname

#Checar qual versão do Linux (32 ou 64 bits) vc está usando:
uname -a

--paste

#Merge files as different collumns:
paste file1 file2 > 2collumns.txt

--less

#See line numbers:
less -N file.txt

--chsh

#Change SHELL (from /bin/sh to /bin/bash, por exemplo)

sudo chsh -s /bin/bash <username>

(alternativamente, vc pode adicionar a linha "/bin/bash" no fim do arquivo  ~/.profile

--chmod

chmod [ugoa][+-=][rwxXst] filename


#For directories, use -R (ex: grant execute to all):

chmod -R +x dir


--scp

#scp (mandar arquivo pra outra máquina, sem entrar nela)
scp -P 2019  PGRACILIS_FILTERED_DENOVO_K37_out.ace gabriel@ip_remotehost:~
scp -P 2019 -r PGRACILIS_FILTERED_DENOVO_K37 gabriel@ip_remotehost:~ (para diretório)


#scp (pegar arquivo de outra máquina, sem entrar nela)
scp -P 2019 gabriel@ip_remotehost:/path/to/file.txt /some/local/directory

Copy multiple files from the remote host to your current directory on the local host
scp your_username@remotehost.edu:~/\{foo.txt,bar.txt\} .


--trimmomatic

java -jar trimmomatic-0.35.jar PE -threads 12 -phred33 input_forward.fq.gz input_reverse.fq.gz -baseout basename.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36


--grep

# usando grep para identificar padrão no fim da string

ls -R | grep '\.rar$'
$ é o REGEX q indica o fim da string.


Grep apenas para marcar matches no texto:
grep -z filename.txt


--lscpu

#informações sobre cpu (incluindo número de processadores):
lscpu

--lsb-realease

#Checar qual versão de Linux OS está instalada:
lsb_release -a

--miraconvert

miraconvert  -f <input_format> -t <output_format> Arquivo.informat Arquivo.outformat
miraconvert  -f maf -t ace -r C -r f PGRACILIS_AGORAVAI_out.maf PGRACILIS_AGORAVAI_out.ace
miraconvert -f maf -t fasta -r C -r f lniger-REFERENCE_out.maf lniger-ACGT


--SRAtoolkit

Baixar arquivo .sra (baixa para ~/ncbi/public/sra)
path/to/prefetch <accession number>

Converter sra em fastq (substituir split-spot por split-3 ou split-files para R1 e R2):
nohup /bioinfo/sratoolkit.2.8.1-centos_linux64/bin/fastq-dump.2.8.1 --split-spot --clip --read-filter pass --skip-technical --readids --dumpbase SRR1742979.Pgracilis.sra &

-X 10000 - Imprime os primeiros 10000 spots.

--origfmt - O cabeçalho fica no formato Illumina

CHANGE HEADER:

fastq-dump --split-files --defline-seq '@$ac-$sn/$ri' --defline-qual '+' -O ./ ACCESSION




--NOVOPlasty

Chama o script e diz onde está o arquivo de configuração.
nohup perl ~/bioinfo/NOVOPlasty-master/NOVOPlasty2.6.pl -c config.txt > NOVOPlasty.out 2> NOVOPlasty.err &

config.txt :

Insert_size: SRA run selector.
Read Length: só dar um less no fast
SEED: procurar gene mitocondrial da espécie ou gênero (usar filt "genetic compartiments" do genbank)




--MITObim

nohup /bioinfo/MITObim-master/MITObim_1.8.pl -end 40 --clean -sample pgracilis -ref REFERENCE -readpool ../SRR1742979.Pgracilis.fastq -maf PGRACILIS_AGORAVAI_d_results/PGRACILIS_AGORAVAI_out.maf > mitobim.out 2> mitobim.err &

Interleaved fastq (para rodar paired end):
/bioinfo/MITObim-master/misc_scripts/interleave-fastqgz-MITOBIM.py SRR5150667.Taethiops_1.fastq SRR5150667.Taethiops_2.fastq > SRR5150667.Taethiops_interleaved.fastq



--fastqc

nohup fastqc -o fastqc_out/ SRR5112522.Pevitus_2.fastq &


--bowtie2

#Bowtie2-build - cria database para o bowtie2

bowtie2-build 13FORM.fasta 13FORM

#Roda BOWTIE2
nohup bowtie2 -x 13FORM -U SRR1743140.Pgracilis.fastq -S SRR1743140.Pgracilis.sam --no-unal --no-head --very-sensitive-local -p 32 &
nohup bowtie2 -x PVENEFICUS -1 SRR5112539.Pmixtecus_1.fastq -2 SRR5112539.Pmixtecus_2.fastq -S SRR5112539.Pmixtecus.sam --no-unal --no-head --very-sensitive-local -p 8 &

#Obtem a lista de ids das reads mapeadas
cat SRR1743140.Pgracilis.sam | awk '{print $1}' | sort -u > readList.info



--genechecker

python /bioinfo/mitoMaker/geneChecker.py ../Srichteri.gb JWHX01003879.fasta JWHX01003879.gb


--spades

nohup /bioinfo/SPAdes-3.10.0-Linux/bin/spades.py -o PPALIDUS_SPADES_DENOVO_2MILLION -1 SRR1742947.Ppallidus.0bp_pass_1.fastq -2 SRR1742947.Ppallidus.0bp_pass_2.fastq --threads 24 --memory 20 -k 61,73,79,89,47,23 >spades.out >spades.err &

--blast

#Creating bastable database:
makeblastdb -in database_file.fa -dbtype prot/nucl -out DATABASE_NAME

#Running blast(p) - tabular:
blastp -db DATABASE_NAME -evalue 1e-4 -query query_sequences.fa -out blast_results -outfmt 6


--kraken2

#Running kraken2 with fastq files:
kraken2 --use-names --threads 4 --db ../16S_RDP_20190418/ --fastq-input --report report_kraken2 --paired FK_T7_S132_L001_R1_001.fastq FK_T7_S132_L001_R2_001.fastq

Database files can be downloaded [here](https://ccb.jhu.edu/software/kraken2/index.shtml?t=downloads)


-- Download Dropbox zip files:

#Need to be extracted using 7za 
